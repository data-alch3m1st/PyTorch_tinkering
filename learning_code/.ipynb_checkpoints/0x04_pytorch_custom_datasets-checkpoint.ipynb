{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4153853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from skorch import NeuralNetClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix\n",
    "    , accuracy_score, ConfusionMatrixDisplay\n",
    "    )\n",
    "from torchmetrics import Accuracy, ConfusionMatrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MacoOS device agnostic code:\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"  # Use NVIDIA GPU (if available)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # Use Apple Silicon GPU (if available)\n",
    "else:\n",
    "    device = \"cpu\"  # Default to CPU if no GPU is available\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d27b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_data_utils_datasets.py\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "def download_file(url: str, dest: Path, chunk_size: int = 1 << 20) -> None:\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "def sha256sum(path: Path, chunk_size: int = 1 << 20) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def extract_zip(zip_path: Path, target_dir: Path) -> None:\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(target_dir)\n",
    "\n",
    "def prepare_vision_dataset(\n",
    "    dataset_name: str\n",
    "    , url: str\n",
    "    , base_dir: Path = Path(\"data\")\n",
    "    , expected_splits: Tuple[str, ...] = (\"train\", \"test\")\n",
    "    , checksum_sha256: Optional[str] = None\n",
    "    , force_download: bool = False\n",
    "    , cleanup_zip: bool = True\n",
    ") -> Dict[str, Path]:\n",
    "    \"\"\"\n",
    "    Downloads and prepares a vision dataset from a zip URL into a standard layout:\n",
    "      base_dir/dataset_name/{train,test}/class_name/*.jpg\n",
    "\n",
    "    Returns a dict with split names -> Path objects.\n",
    "    \"\"\"\n",
    "    dataset_dir = base_dir / dataset_name\n",
    "    zip_path = base_dir / f\"{dataset_name}.zip\"\n",
    "\n",
    "    # If already prepared and not forcing, short-circuit\n",
    "    if dataset_dir.is_dir() and all((dataset_dir / s).is_dir() for s in expected_splits) and not force_download:\n",
    "        return {s: dataset_dir / s for s in expected_splits}\n",
    "\n",
    "    dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Download\n",
    "    if force_download or not zip_path.exists():\n",
    "        download_file(url, zip_path)\n",
    "\n",
    "    # Optional checksum\n",
    "    if checksum_sha256 is not None:\n",
    "        actual = sha256sum(zip_path)\n",
    "        if actual.lower() != checksum_sha256.lower():\n",
    "            raise ValueError(f\"Checksum mismatch for {zip_path}: expected {checksum_sha256}, got {actual}\")\n",
    "\n",
    "    # Extract\n",
    "    extract_zip(zip_path, dataset_dir)\n",
    "\n",
    "    # Optionally clean up the zip\n",
    "    if cleanup_zip and zip_path.exists():\n",
    "        zip_path.unlink()\n",
    "\n",
    "    # Validate expected splits\n",
    "    missing = [s for s in expected_splits if not (dataset_dir / s).is_dir()]\n",
    "    if missing:\n",
    "        raise FileNotFoundError(f\"Missing expected splits {missing} in {dataset_dir}. \"\n",
    "                                f\"Check the zip structure or adjust expected_splits.\")\n",
    "\n",
    "    return {s: dataset_dir / s for s in expected_splits}\n",
    "\n",
    "def summarize_image_folder(root: Path) -> List[Tuple[str, int, int]]:\n",
    "    \"\"\"\n",
    "    Summarize a directory in ImageFolder layout.\n",
    "    Returns a list of tuples: (dirpath, num_subdirs, num_files).\n",
    "    Also prints a readable summary.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        rows.append((dirpath, len(dirnames), len([f for f in filenames if not f.startswith('.')])))\n",
    "\n",
    "    for dp, nd, nf in rows:\n",
    "        print(f\"There are {nd} directories and {nf} images in '{dp}'.\")\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f09ec9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581a48dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa4bcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48655a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc4ca82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
