{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b95cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ec48f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16a23d330>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Parameters\n",
    "BATCH_SIZE = 64\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "EPOCHS = 5\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "MAX_SEQ_LEN = 256\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee4fd8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_imdb_data(data_dir, split):\n",
    "    \"\"\"Loads IMDB reviews from disk. Returns list of (label, text) tuples.\"\"\"\n",
    "    data = []\n",
    "    for label in ('pos', 'neg'):\n",
    "        labeled_dir = os.path.join(data_dir, split, label)\n",
    "        for fname in os.listdir(labeled_dir):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(labeled_dir, fname), encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                    data.append((label, text))\n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "train_data = load_imdb_data('../../ML_Tinkering_Python/data/aclImdb', 'train')\n",
    "test_data = load_imdb_data('../../ML_Tinkering_Python/data/aclImdb', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6556f7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build Vocabulary\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_list):\n",
    "    for label, text in data_list:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_data),\n",
    "    specials=[\"<unk>\", \"<pad>\"],\n",
    "    max_tokens=MAX_VOCAB_SIZE\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d04f5580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Text + Label Preprocessing\n",
    "label_map = {\"neg\": 0, \"pos\": 1}\n",
    "\n",
    "def process_text(text):\n",
    "    tokens = tokenizer(text)\n",
    "    token_ids = vocab(tokens)\n",
    "    if len(token_ids) > MAX_SEQ_LEN:\n",
    "        token_ids = token_ids[:MAX_SEQ_LEN]\n",
    "    else:\n",
    "        token_ids += [vocab[\"<pad>\"]] * (MAX_SEQ_LEN - len(token_ids))\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "def process_label(label):\n",
    "    return torch.tensor(label_map[label], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6850cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create PyTorch Dataset\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = []\n",
    "        for label, text in samples:\n",
    "            self.samples.append((process_text(text), process_label(label)))\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "train_dataset = IMDBDataset(train_data)\n",
    "test_dataset = IMDBDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9085c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Handle Imbalanced Dataset (Weighted Loss)\n",
    "labels = [label.item() for _, label in train_dataset]\n",
    "\n",
    "class_counts = Counter(labels)\n",
    "\n",
    "class_weights = [1.0 / class_counts[i] for i in range(len(class_counts))]\n",
    "\n",
    "sample_weights = [class_weights[label] for label in labels]\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    sample_weights\n",
    "    , num_samples=len(sample_weights)\n",
    "    , replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a039e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset\n",
    "    , batch_size=BATCH_SIZE\n",
    "    , sampler=sampler\n",
    "    , num_workers=0  # <-- Fix: Use single process (avoid pickle issues)\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset\n",
    "    , batch_size=BATCH_SIZE\n",
    "    , shuffle=False\n",
    "    , num_workers=0  # <-- Fix: Use single process (avoid pickle issues)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bca48520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Define Model\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size\n",
    "            , embed_dim\n",
    "            , padding_idx=pad_idx\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim\n",
    "            , hidden_dim\n",
    "            , batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_dim\n",
    "            , num_classes\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        # hidden: (1, batch, hidden_dim)\n",
    "        out = self.fc(hidden[-1])\n",
    "        return out\n",
    "\n",
    "model = SimpleLSTM(\n",
    "    vocab_size=len(vocab)\n",
    "    , embed_dim=EMBED_DIM\n",
    "    , hidden_dim=HIDDEN_DIM\n",
    "    , num_classes=2\n",
    "    , pad_idx=vocab[\"<pad>\"]\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e49668ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Loss and Optimizer\n",
    "# Use weighted cross entropy to deal with class imbalance\n",
    "\n",
    "# weights\n",
    "weights = torch.tensor(\n",
    "    class_weights\n",
    "    , dtype=torch.float\n",
    ").to(DEVICE)\n",
    "\n",
    "# criterion (CEL)\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=weights)\n",
    "\n",
    "# Optimizer (ADAM)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d977291d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLSTM(\n",
       "  (embedding): Embedding(25000, 128, padding_idx=1)\n",
       "  (lstm): LSTM(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.6844, Acc=0.5322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleLSTM(\n",
       "  (embedding): Embedding(25000, 128, padding_idx=1)\n",
       "  (lstm): LSTM(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=0.6842, Acc=0.5410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleLSTM(\n",
       "  (embedding): Embedding(25000, 128, padding_idx=1)\n",
       "  (lstm): LSTM(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=0.6795, Acc=0.5518\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleLSTM(\n",
       "  (embedding): Embedding(25000, 128, padding_idx=1)\n",
       "  (lstm): LSTM(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss=0.5839, Acc=0.6693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleLSTM(\n",
       "  (embedding): Embedding(25000, 128, padding_idx=1)\n",
       "  (lstm): LSTM(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=0.3127, Acc=0.8754\n"
     ]
    }
   ],
   "source": [
    "# 10. Training Loop\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = batch_x.to(DEVICE)\n",
    "        batch_y = batch_y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "        preds = output.argmax(dim=1)\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total += batch_x.size(0)\n",
    "    acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}: Loss={total_loss/total:.4f}, Acc={acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47e49248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLSTM(\n",
       "  (embedding): Embedding(25000, 128, padding_idx=1)\n",
       "  (lstm): LSTM(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.85      0.81      0.83     12500\n",
      "         pos       0.82      0.85      0.84     12500\n",
      "\n",
      "    accuracy                           0.83     25000\n",
      "   macro avg       0.83      0.83      0.83     25000\n",
      "weighted avg       0.83      0.83      0.83     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11. Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x = batch_x.to(DEVICE)\n",
    "        batch_y = batch_y.to(DEVICE)\n",
    "        output = model(batch_x)\n",
    "        preds = output.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"neg\", \"pos\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ca4c5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10150,  2350],\n",
       "       [ 1852, 10648]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(all_labels, all_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
