{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13aa7dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746039de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16a23d330>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Parameters\n",
    "BATCH_SIZE = 64\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "EPOCHS = 5\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "MAX_SEQ_LEN = 256\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97f6b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_imdb_data(data_dir, split):\n",
    "    \"\"\"Loads IMDB reviews from disk. Returns list of (label, text) tuples.\"\"\"\n",
    "    data = []\n",
    "    for label in ('pos', 'neg'):\n",
    "        labeled_dir = os.path.join(data_dir, split, label)\n",
    "        for fname in os.listdir(labeled_dir):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(labeled_dir, fname), encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                    data.append((label, text))\n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "train_data = load_imdb_data('../../ML_Tinkering_Python/data/aclImdb', 'train')\n",
    "test_data = load_imdb_data('../../ML_Tinkering_Python/data/aclImdb', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c334a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build Vocabulary\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(data_list):\n",
    "    for label, text in data_list:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_data),\n",
    "    specials=[\"<unk>\", \"<pad>\"],\n",
    "    max_tokens=MAX_VOCAB_SIZE\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08851544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Text + Label Preprocessing\n",
    "label_map = {\"neg\": 0, \"pos\": 1}\n",
    "\n",
    "def process_text(text):\n",
    "    tokens = tokenizer(text)\n",
    "    token_ids = vocab(tokens)\n",
    "    if len(token_ids) > MAX_SEQ_LEN:\n",
    "        token_ids = token_ids[:MAX_SEQ_LEN]\n",
    "    else:\n",
    "        token_ids += [vocab[\"<pad>\"]] * (MAX_SEQ_LEN - len(token_ids))\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "def process_label(label):\n",
    "    return torch.tensor(label_map[label], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6473a8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create PyTorch Dataset\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = []\n",
    "        for label, text in samples:\n",
    "            self.samples.append((process_text(text), process_label(label)))\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "train_dataset = IMDBDataset(train_data)\n",
    "test_dataset = IMDBDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7a8a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Handle Imbalanced Dataset (Weighted Loss)\n",
    "labels = [label.item() for _, label in train_dataset]\n",
    "\n",
    "class_counts = Counter(labels)\n",
    "\n",
    "class_weights = [1.0 / class_counts[i] for i in range(len(class_counts))]\n",
    "\n",
    "sample_weights = [class_weights[label] for label in labels]\n",
    "\n",
    "sampler = WeightedRandomSampler(\n",
    "    sample_weights\n",
    "    , num_samples=len(sample_weights)\n",
    "    , replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "503ec462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset\n",
    "    , batch_size=BATCH_SIZE\n",
    "    , sampler=sampler\n",
    "    , num_workers=2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset\n",
    "    , batch_size=BATCH_SIZE\n",
    "    , shuffle=False\n",
    "    , num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1d84430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Define Model\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size\n",
    "            , embed_dim\n",
    "            , padding_idx=pad_idx\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim\n",
    "            , hidden_dim\n",
    "            , batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_dim\n",
    "            , num_classes\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        # hidden: (1, batch, hidden_dim)\n",
    "        out = self.fc(hidden[-1])\n",
    "        return out\n",
    "\n",
    "model = SimpleLSTM(\n",
    "    vocab_size=len(vocab)\n",
    "    , embed_dim=EMBED_DIM\n",
    "    , hidden_dim=HIDDEN_DIM\n",
    "    , num_classes=2\n",
    "    , pad_idx=vocab[\"<pad>\"]\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f386b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Loss and Optimizer\n",
    "# Use weighted cross entropy to deal with class imbalance\n",
    "\n",
    "# weights\n",
    "weights = torch.tensor(\n",
    "    class_weights\n",
    "    , dtype=torch.float\n",
    ").to(DEVICE)\n",
    "\n",
    "# criterion (CEL)\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    weight=weights)\n",
    "\n",
    "# Optimizer (ADAM)\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d5058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLSTM(\n",
       "  (embedding): Embedding(25000, 128, padding_idx=1)\n",
       "  (lstm): LSTM(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/t4ng0_br4v0/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/t4ng0_br4v0/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'IMDBDataset' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "# 10. Training Loop\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x = batch_x.to(DEVICE)\n",
    "        batch_y = batch_y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "        preds = output.argmax(dim=1)\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total += batch_x.size(0)\n",
    "    acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}: Loss={total_loss/total:.4f}, Acc={acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de157d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x = batch_x.to(DEVICE)\n",
    "        batch_y = batch_y.to(DEVICE)\n",
    "        output = model(batch_x)\n",
    "        preds = output.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"neg\", \"pos\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018fd0d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
