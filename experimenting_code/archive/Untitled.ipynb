{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, csv, time\n",
    "from pathlib import Path\n",
    "from contextlib import nullcontext\n",
    "\n",
    "results_path = Path(\"./grid_results.csv\")\n",
    "results_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Choose strategy\n",
    "use_random_search = True\n",
    "max_configs = 24  # if random search\n",
    "k_folds = 2       # reduce K\n",
    "epochs_max = 6    # allow higher max but prune early\n",
    "warmup_epochs = 1 # evaluate/prune after this many epochs\n",
    "early_margin = 0.01  # prune if clearly below best\n",
    "\n",
    "# Update param_grid epochs if you like, but we'll override per-run\n",
    "param_grid = {\n",
    "    \"channels\": [32, 64],\n",
    "    \"dropout\": [0.0, 0.25, 0.5],\n",
    "    \"lr\": [6e-4, 1e-3, 6e-3, 1e-2],\n",
    "    \"weight_decay\": [0.0, 1e-4],\n",
    "    \"batch_size\": [256],      # increase if possible\n",
    "    \"optimizer\": [\"adam\"],    # simplify for speed\n",
    "    \"epochs\": [epochs_max],\n",
    "}\n",
    "\n",
    "# Build param list\n",
    "all_params = list(param_product(param_grid))\n",
    "if use_random_search and len(all_params) > max_configs:\n",
    "    rng = np.random.default_rng(27)\n",
    "    all_params = list(rng.choice(all_params, size=max_configs, replace=False))\n",
    "\n",
    "# Precompute folds once\n",
    "folds = list(kfold_indices(len(train_full), k=k_folds, seed=27))\n",
    "\n",
    "# Build static val loaders per fold (batch_size large, no shuffle)\n",
    "val_loaders = []\n",
    "for _, val_idx in folds:\n",
    "    val_ds = Subset(train_full, val_idx)\n",
    "    val_loaders.append(DataLoader(\n",
    "        val_ds, batch_size=512, shuffle=False,\n",
    "        num_workers=4, pin_memory=(device==\"cuda\"),\n",
    "        persistent_workers=True\n",
    "    ))\n",
    "\n",
    "# Utility: CSV logger\n",
    "def log_result(row_dict):\n",
    "    exists = results_path.exists()\n",
    "    with results_path.open(\"a\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=row_dict.keys())\n",
    "        if not exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row_dict)\n",
    "\n",
    "# Utility: check if a (params, fold, epochs_done) result exists\n",
    "def result_key(p, fold):\n",
    "    # Make a stable, hashable string\n",
    "    key = json.dumps({**p, \"fold\": fold}, sort_keys=True)\n",
    "    return key\n",
    "\n",
    "# AMP context\n",
    "use_amp = (device == \"cuda\") or (device == \"mps\")\n",
    "autocast_dtype = torch.float16 if device == \"cuda\" else torch.bfloat16\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n",
    "\n",
    "# Optional: cuDNN speed\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "best_score_global = -1.0\n",
    "best_params = None\n",
    "\n",
    "# Cache built train loaders per (fold, batch_size)\n",
    "train_loaders_cache = {}\n",
    "\n",
    "for p_idx, params in enumerate(all_params):\n",
    "    start_config = time.time()\n",
    "    cv_scores = []\n",
    "    pruned = False\n",
    "\n",
    "    # Track config best across folds for pruning\n",
    "    config_fold_scores = []\n",
    "\n",
    "    for f_idx, (train_idx, val_idx) in enumerate(folds):\n",
    "        # Build/reuse train loader for this batch size\n",
    "        key = (f_idx, params[\"batch_size\"])\n",
    "        if key not in train_loaders_cache:\n",
    "            train_ds = Subset(train_full, train_idx)\n",
    "            train_loaders_cache[key] = DataLoader(\n",
    "                train_ds, batch_size=params[\"batch_size\"], shuffle=True,\n",
    "                num_workers=4, pin_memory=(device==\"cuda\"),\n",
    "                persistent_workers=True\n",
    "            )\n",
    "        train_loader = train_loaders_cache[key]\n",
    "        val_loader = val_loaders[f_idx]\n",
    "\n",
    "        # Fresh model\n",
    "        model = SimpleCNN(channels=params[\"channels\"], dropout=params[\"dropout\"]).to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = make_optimizer(params[\"optimizer\"], model.parameters(),\n",
    "                                   lr=params[\"lr\"], weight_decay=params[\"weight_decay\"])\n",
    "\n",
    "        # Train with AMP + early pruning\n",
    "        best_val_this_fold = -1.0\n",
    "        for epoch in range(epochs_max):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            total = 0\n",
    "\n",
    "            for X, y in train_loader:\n",
    "                X = X.to(device, non_blocking=True)\n",
    "                y = y.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                if use_amp:\n",
    "                    with torch.autocast(device_type=(\"cuda\" if device==\"cuda\" else \"cpu\"),\n",
    "                                        dtype=autocast_dtype):\n",
    "                        logits = model(X)\n",
    "                        loss = criterion(logits, y)\n",
    "                    if device == \"cuda\":\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                else:\n",
    "                    logits = model(X)\n",
    "                    loss = criterion(logits, y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                running_loss += loss.item() * X.size(0)\n",
    "                total += y.size(0)\n",
    "\n",
    "            # Validate\n",
    "            val_loss, val_acc = evaluate(model, val_loader, criterion=None)\n",
    "            best_val_this_fold = max(best_val_this_fold, val_acc)\n",
    "            # Log after each epoch for resumability/diagnostics\n",
    "            log_result({\n",
    "                \"time\": time.time(),\n",
    "                \"config_idx\": p_idx,\n",
    "                \"fold\": f_idx,\n",
    "                \"epoch\": epoch+1,\n",
    "                \"val_acc\": float(val_acc),\n",
    "                \"train_loss_epoch_avg\": running_loss / max(total, 1),\n",
    "                \"params\": json.dumps(params, sort_keys=True)\n",
    "            })\n",
    "\n",
    "            # Prune after warmup if clearly worse than current global best\n",
    "            if epoch+1 >= warmup_epochs and best_score_global > 0:\n",
    "                # Allow a small margin\n",
    "                if val_acc < best_score_global - early_margin:\n",
    "                    # Stop training this fold early\n",
    "                    break\n",
    "\n",
    "        cv_scores.append(best_val_this_fold)\n",
    "\n",
    "        # Optional: fold-level pruning (if average so far will never catch up)\n",
    "        mean_so_far = float(np.mean(cv_scores))\n",
    "        # optimistic max possible if remaining folds reached 1.0\n",
    "        optimistic_max = (mean_so_far * len(cv_scores) + (k_folds - len(cv_scores)) * 1.0) / k_folds\n",
    "        if best_score_global > 0 and optimistic_max < best_score_global - early_margin:\n",
    "            pruned = True\n",
    "            break\n",
    "\n",
    "    if len(cv_scores) == 0:\n",
    "        continue\n",
    "    mean_cv = float(np.mean(cv_scores))\n",
    "    if mean_cv > best_score_global:\n",
    "        best_score_global = mean_cv\n",
    "        best_params = params\n",
    "        print(f\"New best: {best_score_global:.4f} with {best_params}\")\n",
    "\n",
    "    # Summarize config\n",
    "    log_result({\n",
    "        \"time\": time.time(),\n",
    "        \"config_idx\": p_idx,\n",
    "        \"fold\": \"ALL\",\n",
    "        \"epoch\": \"final\",\n",
    "        \"val_acc\": mean_cv,\n",
    "        \"train_loss_epoch_avg\": \"\",\n",
    "        \"params\": json.dumps(params, sort_keys=True)\n",
    "    })\n",
    "\n",
    "print(\"Best CV accuracy:\", best_score_global)\n",
    "print(\"Best params:\", best_params)\n",
    "print(f\"Results saved to {results_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc6a326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0edbba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc4301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb7e52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b589e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7342c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9884d37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6194a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
